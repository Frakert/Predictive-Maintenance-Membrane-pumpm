{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Definitions:","metadata":{}},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom imblearn.over_sampling import SMOTE\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport shap\n\nfrom keras import layers\n\nprint('Importing all done!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Functions","metadata":{}},{"cell_type":"markdown","source":"### automatic_feature_extraction","metadata":{}},{"cell_type":"code","source":"def automatic_feature_extraction(dataset,filepath):\n     # Not tested, last time took 1 hour 15 minutes and didnt feel like sittinng through that again. Result is in dataset\n        \n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Data Cleaning\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    Batch_Names=data['5IAL_3_301.BatchName']\n    data['5IAL_3_301.BatchName']= data['5IAL_3_301.BatchName'].fillna('No Batch Specified')\n    \n    \n    # Filter out everything that is not a batch\n    column_name = '5IAL_3_301.BatchName'\n    mask = (data[column_name].str.len() >= 11) & (data[column_name].str.len() <= 12)\n    data_selected = data[mask]\n    \n    Batch_Names=data['5IAL_3_301.BatchName'][mask]\n    unique_names=pd.DataFrame(Batch_Names.unique(),columns=['5IAL_3_301.BatchName'])\n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Create Target\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    Batch_Names=data['5IAL_3_301.BatchName'][mask] #gets all batches\n    search_strings = ['MD112304504', 'CP7012217502'] # [ 14-02-23:'MD112304504', 06-10-22:'MD112227901', 27-06-22'CP7012217502'] -MD112227901\n    \n    y=unique_names.copy()\n    \n    y['contains_search_string'] = False\n    \n    \n    #check for name wether its in range of one of the search strin names, if so label as True.\n    for i in range(len(y)):\n        for j in range(i+1, min(i+21, len(y))):\n            if any([search_str == y.loc[j, '5IAL_3_301.BatchName'] for search_str in search_strings]):\n                y.loc[i, 'contains_search_string'] = True\n                break\n\n    print(np.sum(y['contains_search_string']))\n    \n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Automatic Feature extraction with tsfresh\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    import tsfresh\n    from tsfresh import extract_features\n    from tsfresh import select_features\n    from tsfresh.utilities.dataframe_functions import impute\n\n    data_selected=data_selected.drop(['5IAL_3_XPV301.36'],axis=1)\n\n    extracted_features = extract_features(data_selected, column_id=\"5IAL_3_301.BatchName\")\n\n    impute(extracted_features)\n\n    y=y.set_index(\"5IAL_3_301.BatchName\")\n\n    features_filtered = select_features(extracted_features, y['contains_search_string'])\n    features_filtered.to_csv(filepath)\n    \n    print('Features saved as csv file to {}'.format(filepath))\n\n    \n    return features_filterd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### clean_data","metadata":{}},{"cell_type":"code","source":"def Clean_data(data, use_PCA):\n    \n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Data Cleaning\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    Batch_Names=data['5IAL_3_301.BatchName']\n    data['5IAL_3_301.BatchName']= data['5IAL_3_301.BatchName'].fillna('No Batch Specified')\n    \n    \n    # Filter out everything that is not a batch\n    column_name = '5IAL_3_301.BatchName'\n    mask = (data[column_name].str.len() >= 11) & (data[column_name].str.len() <= 12)\n    data_selected = data[mask]\n    data=data_selected\n\n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Normalise All data\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \n    Normalise using the z-score\n    \"\"\"\n    BatchName=data['5IAL_3_301.BatchName']\n    Dates=data['Date']\n\n    data.drop(['5IAL_3_301.BatchName','Date'],axis=1,inplace=True)\n\n    data_norm=pd.DataFrame()\n    data_norm_values=pd.DataFrame()\n    for col in data:\n        zval=(data[col]-np.mean(data[col]))/np.std(data[col])\n        data_norm=pd.concat([data_norm,zval],axis=1)\n        data_norm_values[col]=[np.mean(data[col]),np.std(data[col])]\n\n    data_norm_values=data_norm_values.rename(index={0:'mean', 1:'std'})\n    \n    data_norm_values.to_csv('/kaggle/working/normalisation_values.csv')\n        \n    data_norm['5IAL_3_301.BatchName']=BatchName\n    data['5IAL_3_301.BatchName']=BatchName\n    \n    \n    \"\"\" \n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Feature Creation\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    data_norm['frac']=data_norm['5IAL_3_P301.70']/data_norm['5IAL_3_FIT301.61MF']\n\n    \n    Batch_duration=data.groupby(data['5IAL_3_301.BatchName']).size()\n    Batch_duration=pd.DataFrame(Batch_duration, columns=['Batch_duration'])\n    \n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    One-hot Encoder\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    Batch_Names=data['5IAL_3_301.BatchName'][mask]\n    unique_names=pd.DataFrame(Batch_Names.unique(),columns=['5IAL_3_301.BatchName'])\n\n    batch_types_list=[]\n    \n    # for every unique name, check length and choose wether to take the first 3 or 4 letters\n    for i in range(len(unique_names)):\n        if len(unique_names.loc[i][0]) == 11:\n            substring=unique_names.loc[i][0][0:3]\n        else:\n            substring=unique_names.loc[i][0][0:4]\n        batch_types_list.append(substring.upper())\n\n    series=pd.Series(batch_types_list)\n    series= series.replace({'IU7':'IU70'})\n    series_unique=series.unique()\n    \n    One_hot=pd.get_dummies(series)\n\n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Create Target\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\"\n    Batch_Names=data['5IAL_3_301.BatchName'][mask] #gets all batches\n    search_strings = ['MD112304504', 'CP7012217502'] # ['MD112304504','MD112227901', 'CP7012217502'] -MD112227901\n    \n    y=unique_names.copy()\n    \n    y['contains_search_string'] = False\n    \n    \n    #check for name wether its in range of one of the search strin names, if so label as True.\n    for i in range(len(y)):\n        for j in range(i+1, min(i+21, len(y))):\n            if any([search_str == y.loc[j, '5IAL_3_301.BatchName'] for search_str in search_strings]):\n                y.loc[i, 'contains_search_string'] = True\n                break\n\n    print(np.sum(y['contains_search_string']))\n    \n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Create X and y\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\" \n    Eind=pd.merge(pd.merge(mean_grouped,std_grouped,on=['5IAL_3_301.BatchName']), unique_names, on=['5IAL_3_301.BatchName'])\n\n    # Add in Batch duration\n    Eind=pd.merge(Eind,Batch_duration,on=['5IAL_3_301.BatchName'])\n    Eind=pd.merge(Eind,max_grouped,on=['5IAL_3_301.BatchName'])\n    Eind=pd.merge(Eind,min_grouped,on=['5IAL_3_301.BatchName'])\n    Eind=pd.merge(Eind,var_grouped,on=['5IAL_3_301.BatchName'])\n    Eind=pd.merge(Eind,sum_grouped,on=['5IAL_3_301.BatchName'])\n\n    Eind=pd.merge(Eind,Batch_duration,on=['5IAL_3_301.BatchName'])\n    \n    X_t = Eind.drop(['5IAL_3_301.BatchName'],axis=1)\n    X_t=X_t.fillna(0)\n    \n    \n    y = y['contains_search_string']\n    \n    \"\"\"\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    PCA\n    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    \"\"\" \n    from sklearn.decomposition import PCA\n    \n    pca = PCA()\n    X_pca = pca.fit_transform(X_t)\n\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    \n    \n    # the use_PCA setting\n    if use_PCA == True:\n        X=pd.concat([X_pca,One_hot],axis=1)\n    else:\n        X=pd.concat([X_t,One_hot],axis=1)\n\n    return X,y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### devide","metadata":{}},{"cell_type":"code","source":"def devide(x,y):\n    if ((x == 0) | (y == 0)):\n        return 0\n    else: \n        return x/y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confuion","metadata":{}},{"cell_type":"code","source":"def Confusion(y_real,y_pred,silent):\n    from sklearn import metrics \n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    \n    cf=confusion_matrix(y_real,y_pred)\n    TN=cf[0][0]\n    TP=cf[1][1]\n    FP=cf[0][1]\n    FN=cf[1][0]\n\n    Recall=devide(TP,(TP+FN))\n    Precision=devide(TP,(TP+FP))\n    F1=2*devide((Recall*Precision),(Precision+Recall))\n    \n    \n    \n    if silent != 1:\n        print(F1)\n        cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cf, display_labels = [False, True])\n        cm_display.plot()\n        plt.show()\n\n    return(F1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train_fit_Validate_model","metadata":{}},{"cell_type":"code","source":"def Train_fit_Validate_model(X_train,y_train,X_test,y_test,model,plot):\n    model = model\n\n    model.fit(X_train,y_train)\n\n\n    y_fit=model.predict(X_train)\n    y_pred=model.predict(X_test)\n\n    if plot == 1:\n        print('Test Results:')\n        F1=Confusion(y_test,y_pred,silent=0)\n\n        print(' ')\n        print('Train results:')\n        Confusion(y_train,y_fit,silent=0)\n        \n    elif plot == 0:\n        F1=Confusion(y_test,y_pred,silent=1)\n\n    return F1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train_and_test_kfold","metadata":{}},{"cell_type":"code","source":"def train_and_test_kfold(X,y,model,n_splits,plot):\n\n    import numpy as np\n    from sklearn.model_selection import StratifiedKFold\n    from imblearn.over_sampling import SMOTE\n    \n    F1_array=[]\n\n    kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n    for train,test in kf.split(X,y):\n        X_train=X.loc[train]\n        y_train=y.loc[train]\n\n        X_test=X.loc[test]\n        y_test=y.loc[test]\n\n        oversample = SMOTE()\n        X_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n\n\n        F1=Train_fit_Validate_model(X_train_o,y_train_o,X_test,y_test, model=model, plot=plot)\n        F1_array.append(F1)\n    \n    return F1_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Real Work","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/301-330-days/Data_5IAL_3_301.csv',parse_dates=[1],index_col=[0])\ndata.rename(columns={'0':'Date'},inplace=True)\ndata.head()\n\nfeatures=pd.read_csv('/kaggle/input/extracted-features/feature_extraction.csv')\nfeatures.rename(columns={'Unnamed: 0':'5IAL_3_301.BatchName'},inplace=True)\nfeatures.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tsfresh\n\nfeatures_edit=features.set_index('5IAL_3_301.BatchName')\n\nkind_to_fc_parameters = tsfresh.feature_extraction.settings.from_columns(features_edit)\nprint(kind_to_fc_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare X and y","metadata":{}},{"cell_type":"code","source":"X,y = Clean_data(data, False)\n\nX=pd.concat([X,features.drop(['5IAL_3_301.BatchName'],axis=1)],axis=1)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,shuffle=True) #split data into validation and train+test\n\noversample = SMOTE()\nX_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n\nprint(y_train_o.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier #(https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n\nTrain_fit_Validate_model(X_train_o,y_train_o,X_test,y_test,MLPClassifier(max_iter=1000),plot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nTrain_fit_Validate_model(X_train_o,y_train_o,X_test,y_test,RandomForestClassifier(),plot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nmodel_xgb=xgb.XGBClassifier()\n\nF1=Train_fit_Validate_model(X_train_o,y_train_o,X_test,y_test, model=model_xgb, plot=1)\n\nprint(F1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\nmodel=xgb.XGBClassifier(colsample_bytree=0.8, learning_rate=0.075, max_depth= 3, n_estimators= 500)\n\nTest_peformance= train_and_test_kfold(X,y,model=model,n_splits=4,plot=1)\n\nprint(np.mean(Test_peformance))\n\nimport matplotlib.pyplot as plt\n\nplot=Test_peformance.copy()\nplot.append(np.mean(plot))\n\nbar=plt.bar(['1','2','3','4','Average'],plot)\nplt.ylabel('F1-score')\nplt.xlabel('fold number')\nbar[4].set_color('g')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\nlst=[]\nfor i in range(30):\n    Test_peformance= train_and_test_kfold(X,y,model=xgb.XGBClassifier(colsample_bytree=0.8, learning_rate=0.075, max_depth= 3, n_estimators= 500),n_splits=4,plot=0)\n    lst.append(Test_peformance)\n\nprint(np.mean(lst))\nprint(np.std(lst))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"another_list_mean=[]\nanother_list_std=[]\nfor sub_lst in lst:\n    another_list_mean.append(np.mean(sub_lst))\n    another_list_std.append(np.std(sub_lst))\n\nplot=another_list_mean.copy()\nplot.append(np.mean(another_list_mean))\n\nerr=another_list_std.copy()\nerr.append(np.std(lst))\n\nfig, ax = plt.subplots(figsize=(20,10))\n\ncolors = ['#1f77b4'] * 31\ncolors[30] = 'green'\n\nax.bar(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','Average'],plot,yerr=err,capsize=10,color=colors)\nax.set_ylabel('F1-score',fontsize=20)\nax.set_xlabel('fold number',fontsize=20)\nax.tick_params(axis='both', labelsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"params = { 'max_depth': [3,4,5,6,7,8,10,13],\n           'learning_rate': [0.01, 0.05,0.075 ,0.1,0.15],\n           'n_estimators': [100,300,500,700,1000],\n           'colsample_bytree': [0.3,0.4,0.5,0.6,0.7,0.8]}\n           \nxgbr = xgb.XGBClassifier()\nclf = GridSearchCV(estimator=xgbr, \n                   param_grid=params,\n                   scoring='f1')\n\nBest parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.075, 'max_depth': 3, 'n_estimators': 500}","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# X,y = Clean_data(data, False)\n\n# X=pd.concat([X,features.drop(['5IAL_3_301.BatchName'],axis=1)],axis=1)\n\n# params = { 'max_depth': [3,4,5,6,7,8,10,13],\n#            'learning_rate': [0.01, 0.05,0.075 ,0.1,0.15],\n#            'n_estimators': [100,300,500,700,1000],\n#            'colsample_bytree': [0.3,0.4,0.5,0.6,0.7,0.8]}\n# xgbr = xgb.XGBClassifier()\n# clf = GridSearchCV(estimator=xgbr, \n#                    param_grid=params,\n#                    scoring='f1')\n# clf.fit(X, y)\n# print(\"Best parameters:\", clf.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keras Neural Network","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras import regularizers\n\n\n\nDropout_rate=0.2\n\nmodel_keras = Sequential(\n    [\n        Input(shape=(X_train.shape[1])),\n        \n        Dense(1200, activation='relu'),\n        \n        Dropout(rate=Dropout_rate),\n        \n        Dense(1000, activation='relu'),\n        \n        Dropout(rate=Dropout_rate),\n        \n        Dense(800, activation='relu'),\n        \n        Dropout(rate=Dropout_rate),\n        \n        Dense(600, activation='relu'),\n        \n        Dropout(rate=Dropout_rate),\n        \n        Dense(400, activation='relu'),\n        \n        Dropout(rate=Dropout_rate),\n        \n        Dense(200, activation='relu'),\n        \n        Dropout(rate=Dropout_rate),\n        \n        Dense(100, activation='relu'),\n    \n        Dense(1, activation='sigmoid')\n    ])\n\nmodel_keras.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['binary_accuracy'],)\nmodel_keras.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model_keras\n\nhistory=model.fit(\n    X_train_o,\n    y_train_o,\n    epochs=3,\n    validation_split=0.2\n)\n\n\ndecision_boundary=0.9\n\ny_train_pred=(model.predict(X_train)>decision_boundary).astype(int)\ny_test_pred=(model.predict(X_test)> decision_boundary).astype(int)\ny_pred=(model.predict(X)> decision_boundary).astype(int)\n\nprint('Test Results:')\nConfusion(y_test,y_test_pred,silent=0)\n\nprint(' ')\nprint('Train results:')\nConfusion(y_train,y_train_pred,silent=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_o, y_o = oversample.fit_resample(X, y)\n\nmodel=xgb.XGBClassifier(colsample_bytree=0.8, learning_rate=0.075, max_depth= 3, n_estimators= 500)\nmodel.fit(X_o,y_o)\n\ny_pred=model.predict(X)\nprint(np.sum(y_pred))\n\nmodel.save_model('membrane_guess_model.json')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}